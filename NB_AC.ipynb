{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "In this project, I aim to predict the domain such as Archaea, Bacteria, Eukaryota or Virus from the abstract of research papers about proteins taken from the MELINE database by implementing null model, standard Naive Bayes Classifier and its extended algorithm (Log_probability and TF-IDF). \n",
    "   \n",
    "**Datasets:**\n",
    "There are 4000 abstracts with imbalanced classes in trg.csv file; there are 1000 abstracts without classes in tst.csv file.\n",
    "\n",
    "**Representation and Data pre-processing:**\n",
    "First of all, I eliminate the punctuations (i.e. dash and prime), stop words (hard-coding) and words which its length is less than 3 from the original 4000 abstracts, I also singularizes the words. As they are all lowercase characters, I don’t need to convert them to lowercase. For the late model evaluation/validation, because the dataset is imbalanced, I implement stratified train/test split (80:20) based on the propostion of each class before the abstracts’ text processing to avoid data leakage (golden rule of Machine Learning). I randomly choose 3200 abstracts as a training set and extract unique words from those 3200 abstracts as my vocabulary for each run of train/test split (I experiment 100 runs, so I have different vocabulary each run). The vocabulary is considered the features/columns. I mapped the abstracts from both the 3200 training set and 800 test set to the vocabulary with frequencies each word of vocabulary occurs in each abstract. So the row is represented as abstracts, and the column is represented as each word in the vocabulary; the value is represented as the frequencies. We will have both training and test set in this setting of representation.\n",
    "\n",
    "**Implementation of null model:**\n",
    "To find the most frequency class in training dataset, and labels all the test data as the most frequency class.\n",
    "\n",
    "**Implementation of standard Naive Bayes:**\n",
    "As Naïve Bayes classifier assumes, which the event each word occurs is conditionally independent by each other given the class. We use MAP (maximum a posterior) to predict the class.  First, I calculate the prior probabilities for each class. Secondly, I calculate the conditional probabilities, which is the likelihood of each word given each class. I also apply Laplace smoothing to this step to avoid the problem of zero probability. For each abstract, I use the pre-calculated prior probabilities and conditional probabilities to multiply them together for each class (‘A’, ‘B’, ‘E’, ‘V’), and predict the class with the highest product.  \n",
    "\n",
    "**Implementation of Naive Bayes extended with Log_probabillty and TF-IDF:**\n",
    "As the poor performance of the standard Naïve Bayes implementation and its performance is significantly different from the null model, which takes the majority class as predicition, it suffers from underflow issue extremaly, literally, the prodct of probabities is too small, which returns 0 value, so I use log probabilities to fix this issue. Basically, I take a log of each probability - prior probabilities and condiaitonal probabilities. By the handy property of logarithm, I can find the MAP by summation of the log probabilities instead of product of the probabilies. \n",
    "The standard Naïve Bayes also has a problem if a word appears in every abstract which is obviously meaningless for our classification. So I introduce the TF-IDF to my extended algorithm. Bacially I pre-calcualte IDF (inverse document frequency) term initially and take a log of them, I replace the original frequencies term by the product of original requencies and IDF term as new frequencies.  The rest of steps is the same as the standard Naïve Bayes.\n",
    "\n",
    "**Results:**\n",
    "I run stratified cross-validation 100 times and use paired sample t-test to test whether there is a significant difference among the performance of the null model, standard Naive Bayes and the extended version. \n",
    "1. As the result from the **cell [14]**, it is showing there is a significant difference (t_score = -273.191 and df = 99) between extended Naïve Bayes and standard Naïve Bayes , and the **cell [12]** is showing the extended Naïve Bayes (average accuracy = 0.9688) outperforms than the standard Naïve Bayes (average accuracy = 0.5187). \n",
    "2. As the result from the **cell [15]**, it is showing there is a significant difference (t_score = -818.752 and df = 99) between extended Naïve Bayes and null model, and the **cell [12]** is showing the extended Naïve Bayes (average accuracy = 0.9688) outperforms than the null model (average accuracy = 0.5363). \n",
    "3. As the result from the **cell [16]**, it is showing there is a significant difference (t_score = 11.276 and df = 99) between standard Naïve Bayes and null model, and the **cell [12]** is showing the null model (average accuracy = 0.5363) outperforms than the standard Naïve Bayes (average accuracy = 0.5187).\n",
    "\n",
    "**Kaggle submission:**\n",
    "My highest score is 0.97666    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading essential packages\n",
    "- numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'\n",
    "\n",
    "## Loading trg.csv by columns \n",
    "id_trg = []\n",
    "class_trg = []\n",
    "abstracts_trg = []\n",
    "with open(r'C:\\Users\\jovi1\\OneDrive - The University of Auckland\\Documents\\COMPSCI 762_2021\\Assignment 3\\trg.csv', 'r') as f1:\n",
    "    lines = f1.readlines()[1:]\n",
    "    for line in lines:\n",
    "        row = line.split(',')\n",
    "        id_trg.append(row[0])\n",
    "        class_trg.append(row[1])\n",
    "        abstracts_trg.append(''.join([c for c in row[2] if c not in punctuations]))\n",
    "id_trg = np.array(id_trg)\n",
    "class_trg = np.array(class_trg)\n",
    "abstracts_trg = np.array(abstracts_trg)\n",
    "\n",
    "## Loading tst.csv by columns       \n",
    "id_tst = []\n",
    "abstracts_tst = []\n",
    "with open(r'C:\\Users\\jovi1\\OneDrive - The University of Auckland\\Documents\\COMPSCI 762_2021\\Assignment 3\\tst.csv', 'r') as f2:\n",
    "    lines = f2.readlines()[1:]\n",
    "    for line in lines:\n",
    "        row = line.split(',')\n",
    "        id_tst.append(row[0])\n",
    "        abstracts_tst.append(''.join([c for c in row[1] if c not in punctuations]))\n",
    "id_tst = np.array(id_tst)\n",
    "abstracts_tst = np.array(abstracts_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement text processing\n",
    "- Create a dictionary for all the distinct words with their occurrence frequencies.\n",
    "- Remove the stop words from the text in all abstracts.\n",
    "- Remove the digits\n",
    "- Remove the words whose length is less than three\n",
    "- Singularize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singularizer(word):\n",
    "    if len(word) > 3: \n",
    "        if word[-2:] in 'es' and (word[-4:-2] in ['sh', 'ch'] or word[-3] in ['s', 'x', 'z']):\n",
    "            word = word[:-2]\n",
    "        elif word[-1] == 's' and (word[-4:-2] not in ['sh', 'ch'] or word[-3] not in ['s', 'x', 'z']):\n",
    "            word = word[:-1]\n",
    "        elif word[-3:] == 'ies':\n",
    "            word = word.replace('ies', 'y')\n",
    "        elif word[-3:] == 'ves':\n",
    "            word = word.replace('ves', 'f')\n",
    "            if word[-2] == 'i':\n",
    "                word = word.replace('f', 'fe')\n",
    "            else:\n",
    "                pass\n",
    "        elif word[-3:] == 'men':\n",
    "            word = word.replace('men', 'man')\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \n",
    "              \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', \n",
    "              'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \n",
    "              'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "              'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n",
    "              'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "              'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "              'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "              'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "              'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', \n",
    "              'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "              'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should',  \n",
    "              \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", \n",
    "              'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "              \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \n",
    "              \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "\n",
    "def get_vocabulary(abstracts):\n",
    "    unique_words = set()\n",
    "    for abstract in abstracts:\n",
    "        for word in abstract.split():\n",
    "            word = singularizer(word)\n",
    "            if word not in stop_words and not word.isdigit() and len(word) > 2:\n",
    "                unique_words.add(word)\n",
    "\n",
    "    vocab = {}\n",
    "    for index, word in enumerate(sorted(list(unique_words))):\n",
    "        vocab[word] = index\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Vectorizer(abstracts, vocabulary):\n",
    "    row, col, val = [], [], []\n",
    "    for row_index, abstract in enumerate(abstracts):\n",
    "        count_word = {}\n",
    "        for word in abstract.split():\n",
    "            word = singularizer(word)\n",
    "            if word in count_word:\n",
    "                count_word[word] += 1\n",
    "            else:\n",
    "                count_word[word] = 1\n",
    "        for word, count in count_word.items():\n",
    "            col_index = vocabulary.get(word)\n",
    "            if col_index != None:\n",
    "                row.append(row_index)\n",
    "                col.append(col_index)\n",
    "                val.append(count)\n",
    "    mat = np.zeros([len(abstracts), len(vocabulary)])\n",
    "    mat[row, col] = val\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement stratified train/test set split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified train_test_split \n",
    "def train_test_split(X, y, test_size, random_state = 1):\n",
    "    np.random.seed(random_state)\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    test_index = np.array([], dtype='int')\n",
    "    train_index = np.array([], dtype='int')\n",
    "    for i in range(len(unique)): \n",
    "        test_num = int(counts[i] * test_size)\n",
    "        ind = np.where(y == unique[i])[0]\n",
    "        test_i = np.random.choice(ind, test_num, replace=False)\n",
    "        train_i = np.setxor1d(ind, test_i)\n",
    "        test_index = np.append(test_index, test_i)\n",
    "        train_index = np.append(train_index, train_i)\n",
    "    return X[train_index], X[test_index], y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement performance metrics\n",
    "- over-all accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metric(y_test, y_pred):\n",
    "    return (y_test == y_pred).sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null model takes the majority class as class prediction.\n",
    "def null_model(y_train, X_test):\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    y_pred = [unique[np.argmax(counts)]] * len(X_test) \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement Standard/Multinomial Navie Bayes classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayes(X_train, y_train, X_test):\n",
    "    voc_size = X_train.shape[1] \n",
    "    \n",
    "    # calculate the priors for each class\n",
    "    prior_A = np.sum(y_train=='A')/len(y_train)\n",
    "    prior_B = np.sum(y_train=='B')/len(y_train)\n",
    "    prior_E = np.sum(y_train=='E')/len(y_train)\n",
    "    prior_V = np.sum(y_train=='V')/len(y_train)\n",
    "    \n",
    "    n_words_A = np.sum(X_train[y_train=='A'])\n",
    "    n_words_B = np.sum(X_train[y_train=='B'])\n",
    "    n_words_E = np.sum(X_train[y_train=='E'])\n",
    "    n_words_V = np.sum(X_train[y_train=='V'])\n",
    "    \n",
    "    # calculate the conditional probability\n",
    "    # Add one to the numerator and add the number of vocabulary to elminate zeros \n",
    "    con_prob_A = (np.sum(X_train[y_train=='A'], axis=0) + 1) / (n_words_A + voc_size)\n",
    "    con_prob_B = (np.sum(X_train[y_train=='B'], axis=0) + 1) / (n_words_B + voc_size)\n",
    "    con_prob_E = (np.sum(X_train[y_train=='E'], axis=0) + 1) / (n_words_E + voc_size)\n",
    "    con_prob_V = (np.sum(X_train[y_train=='V'], axis=0) + 1) / (n_words_V + voc_size)\n",
    "    \n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        ind = np.nonzero(X_test[i])[0]\n",
    "\n",
    "        score_A = prior_A * np.prod(np.power(con_prob_A[ind], X_test[i][ind]))\n",
    "        score_B = prior_B * np.prod(np.power(con_prob_B[ind], X_test[i][ind]))\n",
    "        score_E = prior_E * np.prod(np.power(con_prob_E[ind], X_test[i][ind]))\n",
    "        score_V = prior_V * np.prod(np.power(con_prob_V[ind], X_test[i][ind]))\n",
    "\n",
    "        if max([score_A, score_B, score_E, score_V]) == score_A:\n",
    "            y_pred.append('A')\n",
    "        elif max([score_A, score_B, score_E, score_V]) == score_B:\n",
    "            y_pred.append('B')\n",
    "        elif max([score_A, score_B, score_E, score_V]) == score_E:\n",
    "            y_pred.append('E')\n",
    "        else:\n",
    "            y_pred.append('V')\n",
    "\n",
    "    return np.array(y_pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier extended with log probability and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayes_ext(X_train, y_train, X_test):\n",
    "    voc_size = X_train.shape[1] \n",
    "    \n",
    "    # Calculate IDF term\n",
    "    IDF = np.log(len(X_train) / np.sum(X_train!=0, axis=0))  \n",
    "    \n",
    "    # Calculate the priors for each class\n",
    "    prior_A = np.sum(y_train=='A')/len(y_train)\n",
    "    prior_B = np.sum(y_train=='B')/len(y_train)\n",
    "    prior_E = np.sum(y_train=='E')/len(y_train)\n",
    "    prior_V = np.sum(y_train=='V')/len(y_train)\n",
    "    \n",
    "    n_words_A = np.sum(X_train[y_train=='A'] * IDF)\n",
    "    n_words_B = np.sum(X_train[y_train=='B'] * IDF)\n",
    "    n_words_E = np.sum(X_train[y_train=='E'] * IDF)\n",
    "    n_words_V = np.sum(X_train[y_train=='V'] * IDF)\n",
    "    \n",
    "    # calculate the conditional probability\n",
    "    # Add one to the numerator and add the number of vocabulary to elminate zeros \n",
    "    con_prob_A = (np.sum(X_train[y_train=='A'], axis=0) * IDF + 1) / (n_words_A + voc_size)\n",
    "    con_prob_B = (np.sum(X_train[y_train=='B'], axis=0) * IDF + 1) / (n_words_B + voc_size)\n",
    "    con_prob_E = (np.sum(X_train[y_train=='E'], axis=0) * IDF + 1) / (n_words_E + voc_size)\n",
    "    con_prob_V = (np.sum(X_train[y_train=='V'], axis=0) * IDF + 1) / (n_words_V + voc_size)\n",
    "    \n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        ind = np.nonzero(X_test[i])[0]\n",
    "        score_A = np.log10(prior_A) + np.sum(np.multiply(np.log10(con_prob_A[ind]), X_test[i][ind]))\n",
    "        score_B = np.log10(prior_B) + np.sum(np.multiply(np.log10(con_prob_B[ind]), X_test[i][ind]))\n",
    "        score_E = np.log10(prior_E) + np.sum(np.multiply(np.log10(con_prob_E[ind]), X_test[i][ind]))\n",
    "        score_V = np.log10(prior_V) + np.sum(np.multiply(np.log10(con_prob_V[ind]), X_test[i][ind]))\n",
    "        if max([score_A, score_B, score_E, score_V]) == score_A:\n",
    "            y_pred.append('A')\n",
    "        elif max([score_A, score_B, score_E, score_V]) == score_B:\n",
    "            y_pred.append('B')\n",
    "        elif max([score_A, score_B, score_E, score_V]) == score_E:\n",
    "            y_pred.append('E')\n",
    "        else:\n",
    "            y_pred.append('V')\n",
    "\n",
    "    return np.array(y_pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "- 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = []\n",
    "nb = []\n",
    "nb_ext = []\n",
    "\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(abstracts_trg, class_trg, test_size=0.2, random_state=i)\n",
    "    \n",
    "    vocabulary = get_vocabulary(X_train)\n",
    "    \n",
    "    X_train_cv = Count_Vectorizer(X_train, vocabulary)\n",
    "    X_test_cv = Count_Vectorizer(X_test, vocabulary)\n",
    "    \n",
    "    y_null = null_model(y_train, X_test)\n",
    "    null.append(performance_metric(y_test, y_null))\n",
    "    \n",
    "    y_nb = Naive_Bayes(X_train_cv, y_train, X_test_cv)\n",
    "    nb.append(performance_metric(y_test, y_nb))\n",
    "\n",
    "    y_nb_ext = Naive_Bayes_ext(X_train_cv, y_train, X_test_cv)\n",
    "    nb_ext.append(performance_metric(y_test, y_nb_ext))\n",
    "\n",
    "    \n",
    "null = np.array(null)  \n",
    "nb = np.array(nb)\n",
    "nb_ext = np.array(nb_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracies for null model is 0.5363, and the standard devation is 0.0\n",
      "The average of accuracies for standard naive bayes is 0.5187, and the standard devation is 0.0155\n",
      "The average of accuracies for extended version of naive bayes is 0.9688, and the standard devation is 0.0053\n"
     ]
    }
   ],
   "source": [
    "print('The average of accuracies for null model is {}, and the standard devation is {}'.format(round(np.mean(null), 4), round(np.std(null), 4)))\n",
    "print('The average of accuracies for standard naive bayes is {}, and the standard devation is {}'.format(round(np.mean(nb), 4), round(np.std(nb), 4)))\n",
    "print('The average of accuracies for extended version of naive bayes is {}, and the standard devation is {}'.format(round(np.mean(nb_ext), 4), round(np.std(nb_ext), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paired sample t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_sample_t_score(sample_1, sample_2, size = 100):\n",
    "    mean_1 = np.mean(sample_1)\n",
    "    mean_2 = np.mean(sample_2)\n",
    "    var_1 = np.sum((sample_1 - mean_1) ** 2) / (size - 1)\n",
    "    var_2 = np.sum((sample_2 - mean_2) ** 2) / (size - 1)\n",
    "    std = np.sqrt((var_1 + var_2) / 2)\n",
    "    t_score = (mean_1 - mean_2) / (std * np.sqrt(2 / size))\n",
    "    return t_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison between extended version and standard Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t score: -273.191\n"
     ]
    }
   ],
   "source": [
    "t_score = paired_sample_t_score(nb, nb_ext)\n",
    "print('t score: {}'.format(round(t_score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.95 significant level, the p-value is < 0.00001 with degree of freedom = 99. So we can conclude the Navie Bayes with log probability and TF-IDF outperforms the standard Navie Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison between extended version and null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t score: -818.752\n"
     ]
    }
   ],
   "source": [
    "t_score = paired_sample_t_score(null, nb_ext)\n",
    "print('t score: {}'.format(round(t_score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.95 significant level, the p-value is < 0.00001 with degree of freedom = 99. So we can conclude the Navie Bayes with log probability and TF-IDF outperforms the null model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison between standard Naive Bayes and null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t score: 11.276\n"
     ]
    }
   ],
   "source": [
    "t_score = paired_sample_t_score(null, nb)\n",
    "print('t score: {}'.format(round(t_score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.95 significant level, the p-value is < 0.00001 with degree of freedom = 99. So we can conclude the null model outperforms the standard Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(abstracts_trg)\n",
    "X_tst = Count_Vectorizer(abstracts_tst, vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Count_Vectorizer(abstracts_trg, vocabulary=vocabulary)\n",
    "y = class_trg\n",
    "y_pred = Naive_Bayes_ext(X, y, X_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the output as csv file, and upload it for Kaggle Competition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array([id_tst, y_pred]).T\n",
    "output = np.vstack((['id', 'class'], output))\n",
    "\n",
    "with open('output.csv', 'w') as f3:\n",
    "    f3.write(\"\\n\".join(\",\".join(map(str, x)) for x in output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
