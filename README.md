## Rank 6 of 325 in the Kaggle competation. (https://www.kaggle.com/c/naivebayes-21/leaderboard)

# Naive-Bayes-Abstract-Classification
Naive Bayes (from scratch) Abstract Classification

Goal: In this project, I aim to predict the domain such as Archaea, Bacteria, Eukaryota or Virus from the abstract of research papers about proteins taken from the MELINE database by implementing null model, standard Naive Bayes Classifier and its extended algorithm (Log_probability and TF-IDF).

Datasets: There are 4000 abstracts with imbalanced classes in trg.csv file; there are 1000 abstracts without classes in tst.csv file.

Representation and Data pre-processing: First of all, I eliminate the punctuations (i.e. dash and prime), stop words (hard-coding) and words which its length is less than 3 from the original 4000 abstracts, I also singularizes the words. As they are all lowercase characters, I don’t need to convert them to lowercase. For the late model evaluation/validation, because the dataset is imbalanced, I implement stratified train/test split (80:20) based on the propostion of each class before the abstracts’ text processing to avoid data leakage (golden rule of Machine Learning). I randomly choose 3200 abstracts as a training set and extract unique words from those 3200 abstracts as my vocabulary for each run of train/test split (I experiment 100 runs, so I have different vocabulary each run). The vocabulary is considered the features/columns. I mapped the abstracts from both the 3200 training set and 800 test set to the vocabulary with frequencies each word of vocabulary occurs in each abstract. So the row is represented as abstracts, and the column is represented as each word in the vocabulary; the value is represented as the frequencies. We will have both training and test set in this setting of representation.

Implementation of null model: To find the most frequency class in training dataset, and labels all the test data as the most frequency class.

Implementation of standard Naive Bayes: As Naïve Bayes classifier assumes, which the event each word occurs is conditionally independent by each other given the class. We use MAP (maximum a posterior) to predict the class. First, I calculate the prior probabilities for each class. Secondly, I calculate the conditional probabilities, which is the likelihood of each word given each class. I also apply Laplace smoothing to this step to avoid the problem of zero probability. For each abstract, I use the pre-calculated prior probabilities and conditional probabilities to multiply them together for each class (‘A’, ‘B’, ‘E’, ‘V’), and predict the class with the highest product.

Implementation of Naive Bayes extended with Log_probabillty and TF-IDF: As the poor performance of the standard Naïve Bayes implementation and its performance is significantly different from the null model, which takes the majority class as predicition, it suffers from underflow issue extremaly, literally, the prodct of probabities is too small, which returns 0 value, so I use log probabilities to fix this issue. Basically, I take a log of each probability - prior probabilities and condiaitonal probabilities. By the handy property of logarithm, I can find the MAP by summation of the log probabilities instead of product of the probabilies. The standard Naïve Bayes also has a problem if a word appears in every abstract which is obviously meaningless for our classification. So I introduce the TF-IDF to my extended algorithm. Bacially I pre-calcualte IDF (inverse document frequency) term initially and take a log of them, I replace the original frequencies term by the product of original requencies and IDF term as new frequencies. The rest of steps is the same as the standard Naïve Bayes.
